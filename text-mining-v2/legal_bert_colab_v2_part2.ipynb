{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legal Document Classification with BERT - V2 (Full Dataset)\n",
    "\n",
    "## Part 2: Data Exploration\n",
    "\n",
    "Explore and visualize the full 45K dataset to better understand its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the dataset (use the path from Part 1)\n",
    "dataset_path = '/content/drive/MyDrive/legal_bert_classification_v2/full_bert_dataset.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of unique labels: {df['label'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display label distribution\n",
    "print(\"Label distribution:\")\n",
    "label_counts = df['label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "# Calculate percentages\n",
    "label_percentages = df['label'].value_counts(normalize=True) * 100\n",
    "print(\"\\nLabel percentages:\")\n",
    "for label, percentage in label_percentages.items():\n",
    "    print(f\"  {label}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot label distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "\n",
    "# Add count labels on top of bars\n",
    "for i, count in enumerate(label_counts.values):\n",
    "    ax.text(i, count + 100, f\"{count:,}\", ha='center')\n",
    "\n",
    "plt.title('Document Type Distribution', fontsize=14)\n",
    "plt.xlabel('Document Type', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Text length analysis\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(word_tokenize(x[:10000])))\n",
    "\n",
    "print(\"\\nText length statistics (characters):\")\n",
    "print(df['text_length'].describe())\n",
    "\n",
    "print(\"\\nWord count statistics:\")\n",
    "print(df['word_count'].describe())\n",
    "\n",
    "print(\"\\nText length by label (mean characters):\")\n",
    "print(df.groupby('label')['text_length'].mean().sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nWord count by label (mean words):\")\n",
    "print(df.groupby('label')['word_count'].mean().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot text length distribution by document type\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Create a box plot to show distribution\n",
    "sns.boxplot(x='label', y='text_length', data=df)\n",
    "plt.title('Text Length Distribution by Document Type', fontsize=14)\n",
    "plt.xlabel('Document Type', fontsize=12)\n",
    "plt.ylabel('Text Length (characters)', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, df['text_length'].quantile(0.95))  # Limit y-axis to 95th percentile\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Histogram of text lengths\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='text_length', hue='label', bins=50, element='step', log_scale=(False, True))\n",
    "plt.title('Text Length Distribution by Document Type (Log Scale)', fontsize=14)\n",
    "plt.xlabel('Text Length (characters)', fontsize=12)\n",
    "plt.ylabel('Count (log scale)', fontsize=12)\n",
    "plt.xlim(0, df['text_length'].quantile(0.99))  # Limit x-axis to 99th percentile\n",
    "plt.legend(title='Document Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for potential BERT max token limit issues\n",
    "# BERT typically has a 512 token limit\n",
    "# We'll estimate tokens as words / 0.75 (rough approximation)\n",
    "\n",
    "df['estimated_tokens'] = df['word_count'] / 0.75\n",
    "exceeding_limit = df[df['estimated_tokens'] > 512]\n",
    "\n",
    "print(f\"Number of documents potentially exceeding BERT's 512 token limit: {len(exceeding_limit)} ({len(exceeding_limit)/len(df)*100:.2f}%)\")\n",
    "print(\"\\nDistribution by label:\")\n",
    "print(exceeding_limit['label'].value_counts())\n",
    "print(\"\\nPercentage by label:\")\n",
    "for label in df['label'].unique():\n",
    "    total = len(df[df['label'] == label])\n",
    "    exceeding = len(exceeding_limit[exceeding_limit['label'] == label])\n",
    "    print(f\"  {label}: {exceeding/total*100:.2f}% ({exceeding}/{total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample document display\n",
    "print(\"\\nSample document from each class:\")\n",
    "for label in df['label'].unique():\n",
    "    sample = df[df['label'] == label].iloc[0]\n",
    "    print(f\"\\n--- {label} Example ---\")\n",
    "    print(f\"Text length: {len(sample['text'])} characters\")\n",
    "    print(f\"Word count: ~{len(word_tokenize(sample['text'][:10000]))} words\")\n",
    "    print(f\"Text (first 300 chars): {sample['text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for potential data quality issues\n",
    "# 1. Very short texts\n",
    "short_texts = df[df['text_length'] < 100]\n",
    "print(f\"Number of very short texts (<100 chars): {len(short_texts)}\")\n",
    "if len(short_texts) > 0:\n",
    "    print(\"Sample of short texts:\")\n",
    "    print(short_texts[['text', 'label']].head(3))\n",
    "\n",
    "# 2. Check text format consistency\n",
    "print(\"\\nChecking for text format consistency...\")\n",
    "has_linebreaks = df['text'].apply(lambda x: '\\n' in x).mean() * 100\n",
    "print(f\"Percentage of texts with line breaks: {has_linebreaks:.2f}%\")\n",
    "\n",
    "# 3. Check character distribution\n",
    "def get_char_types(text_sample):\n",
    "    if pd.isna(text_sample):\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    alphabet = sum(c.isalpha() for c in text_sample)\n",
    "    digits = sum(c.isdigit() for c in text_sample)\n",
    "    spaces = sum(c.isspace() for c in text_sample)\n",
    "    \n",
    "    return alphabet, digits, spaces\n",
    "\n",
    "# Sample 1000 random texts for character analysis\n",
    "sample_df = df.sample(min(1000, len(df)))\n",
    "sample_stats = sample_df['text'].apply(get_char_types)\n",
    "\n",
    "alphabet_chars = np.mean([x[0] for x in sample_stats])\n",
    "digit_chars = np.mean([x[1] for x in sample_stats])\n",
    "space_chars = np.mean([x[2] for x in sample_stats])\n",
    "total = alphabet_chars + digit_chars + space_chars\n",
    "\n",
    "print(f\"\\nAverage character composition in sample texts:\")\n",
    "print(f\"  Alphabetic: {alphabet_chars/total*100:.2f}%\")\n",
    "print(f\"  Digits: {digit_chars/total*100:.2f}%\")\n",
    "print(f\"  Spaces: {space_chars/total*100:.2f}%\")\n",
    "print(f\"  Other (punctuation, etc.): {(1-(alphabet_chars+digit_chars+space_chars)/total)*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Legal Document Classification with BERT - V2 Part 2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
