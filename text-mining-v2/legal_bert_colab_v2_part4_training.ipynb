{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legal Document Classification with BERT - V2 (Full Dataset)\n",
    "\n",
    "## Part 4: Model Training\n",
    "\n",
    "Train the BERT model on the full 45K documents dataset with enhanced techniques to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define training function with early stopping\n",
    "def train_model(model, train_loader, val_loader, device, epochs=4, \n",
    "                learning_rate=2e-5, warmup_steps=0, weight_decay=0.01,\n",
    "                early_stopping_patience=2, save_dir=None):\n",
    "    \"\"\"Train the model with early stopping and gradient accumulation.\"\"\"\n",
    "    \n",
    "    if not save_dir:\n",
    "        save_dir = f\"/content/drive/MyDrive/legal_bert_classification_v2/model_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8, weight_decay=weight_decay)\n",
    "    \n",
    "    # Calculate total training steps\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    \n",
    "    # Prepare scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Mixed precision training\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Track training metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_accuracy = 0\n",
    "    best_model_path = os.path.join(save_dir, \"best_model.pt\")\n",
    "    \n",
    "    # Early stopping variables\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    # Save config for reproducibility\n",
    "    with open(os.path.join(save_dir, \"training_config.txt\"), 'w') as f:\n",
    "        f.write(f\"Epochs: {epochs}\\n\")\n",
    "        f.write(f\"Learning rate: {learning_rate}\\n\")\n",
    "        f.write(f\"Weight decay: {weight_decay}\\n\")\n",
    "        f.write(f\"Warmup steps: {warmup_steps}\\n\")\n",
    "        f.write(f\"Batch size: {next(iter(train_loader))['input_ids'].shape[0]}\\n\")\n",
    "        f.write(f\"Early stopping patience: {early_stopping_patience}\\n\")\n",
    "        f.write(f\"Training samples: {len(train_loader.dataset)}\\n\")\n",
    "        f.write(f\"Validation samples: {len(val_loader.dataset)}\\n\")\n",
    "        f.write(f\"Device: {device}\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Track time\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        # Progress bar for training\n",
    "        progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "        \n",
    "        # Gradient accumulation steps (helps with large models/limited memory)\n",
    "        accumulation_steps = 2\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss / accumulation_steps\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Track loss\n",
    "            total_train_loss += loss.item() * accumulation_steps\n",
    "            \n",
    "            # Update weights every accumulation_steps\n",
    "            if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_loader):\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\"loss\": loss.item() * accumulation_steps})\n",
    "            \n",
    "        # Calculate average loss for the epoch\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Progress bar for validation\n",
    "        progress_bar = tqdm(val_loader, desc=\"Validating\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                # Get predictions\n",
    "                logits = outputs.logits\n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().tolist())\n",
    "                all_labels.extend(labels.cpu().tolist())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = sum(1 for p, l in zip(all_preds, all_labels) if p == l) / len(all_preds)\n",
    "        val_accuracies.append(accuracy)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1} completed in {epoch_time:.2f}s\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Save checkpoint to Drive\n",
    "        checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'val_accuracy': accuracy\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved with accuracy: {accuracy:.4f}\")\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"No improvement for {no_improvement_count} epochs. Best accuracy: {best_val_accuracy:.4f}\")\n",
    "            \n",
    "        # Early stopping\n",
    "        if no_improvement_count >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Final save of the model\n",
    "    model_path = os.path.join(save_dir, \"final_model.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Final model saved to {model_path}\")\n",
    "    \n",
    "    # Save training history\n",
    "    history = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define model parameters\n",
    "save_dir = '/content/drive/MyDrive/legal_bert_classification_v2/model'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Initialize model, optimizer, and scheduler from Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train the model\n",
    "trained_model, history = train_model(\n",
    "    model=model,  # From Part 3\n",
    "    train_loader=train_loader,  # From Part 3\n",
    "    val_loader=val_loader,  # From Part 3\n",
    "    device=device,\n",
    "    epochs=4,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=int(0.1 * len(train_loader) * 4),  # 10% of total steps\n",
    "    weight_decay=0.01,\n",
    "    early_stopping_patience=2,\n",
    "    save_dir=save_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Plot loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_losses'], label='Train Loss')\n",
    "plt.plot(history['val_losses'], label='Validation Loss')\n",
    "plt.title('Loss Curves', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['val_accuracies'], marker='o', linestyle='-', color='green')\n",
    "plt.title('Validation Accuracy', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/legal_bert_classification_v2/training_history.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Export model for local deployment\n",
    "\n",
    "# Save model to Drive in transformers format\n",
    "output_dir = \"/content/drive/MyDrive/legal_bert_classification_v2/final_model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load best model\n",
    "best_model_path = os.path.join(save_dir, \"best_model.pt\")\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Save using Hugging Face's save_pretrained method\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {output_dir}\")\n",
    "print(\"Now you can download the model folder for local inference.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Legal Document Classification with BERT - V2 Part 4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
