{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legal Document Classification with BERT\n",
    "\n",
    "## Part 3: Data Preparation\n",
    "\n",
    "If you need to prepare data from JSON files in Colab, this section provides the necessary code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_bert_dataset(json_folder_path, output_file_path, min_text_length=10):\n",
    "    \"\"\"Extracts header + recitals and document type from JSON files and saves to CSV.\"\"\"\n",
    "    \n",
    "    print(f\"Extracting header + recitals and labels from JSON files...\")\n",
    "    \n",
    "    # Get list of JSON files\n",
    "    json_files = [f for f in os.listdir(json_folder_path) if f.lower().endswith('.json')]\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files. Processing...\")\n",
    "    \n",
    "    # Prepare data storage\n",
    "    data = []\n",
    "    skipped_count = 0\n",
    "    empty_text_count = 0\n",
    "    \n",
    "    # Process files with progress bar\n",
    "    for filename in tqdm(json_files):\n",
    "        file_path = os.path.join(json_folder_path, filename)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                article = json.load(f)\n",
    "            \n",
    "            # Extract document type (label)\n",
    "            doc_type = article.get('type', None)\n",
    "            \n",
    "            # Extract header and recitals\n",
    "            header = article.get('header', '')\n",
    "            recitals = article.get('recitals', '')\n",
    "            \n",
    "            # Skip if missing essential data\n",
    "            if not doc_type or (not header and not recitals):\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Combine header and recitals\n",
    "            if header and recitals:\n",
    "                text = f\"{header}\\n{recitals}\"\n",
    "            elif header:\n",
    "                text = header\n",
    "            else:\n",
    "                text = recitals\n",
    "            \n",
    "            # Skip documents with very short text\n",
    "            if len(text) < min_text_length:\n",
    "                empty_text_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Add to dataset\n",
    "            data.append({\n",
    "                'text': text.strip(),\n",
    "                'label': doc_type,\n",
    "                'celex_id': article.get('celex_id', '')  # Keep ID for reference\n",
    "            })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: Error processing file {filename}: {e}. Skipping.\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if not data:\n",
    "        print(\"No valid data extracted. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nExtracted {len(df)} documents with valid text and labels.\")\n",
    "    print(f\"Skipped {skipped_count} documents missing type or text.\")\n",
    "    print(f\"Skipped {empty_text_count} documents with text shorter than {min_text_length} characters.\")\n",
    "    \n",
    "    # Check label distribution\n",
    "    label_counts = df['label'].value_counts()\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"  {label}: {count} ({count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    print(f\"\\nSaving to {output_file_path}...\")\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Data successfully saved to {output_file_path}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Uncomment and run this cell if you have JSON files uploaded to a folder in Drive\n",
    "# json_folder_path = '/content/drive/MyDrive/legal_bert_classification/dataset_folder'\n",
    "# output_file_path = '/content/drive/MyDrive/legal_bert_classification/bert_classification_dataset.csv'\n",
    "# df = prepare_bert_dataset(json_folder_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Data Exploration\n",
    "\n",
    "Explore and visualize the dataset to better understand its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset (adjust path as needed)\n",
    "# If you uploaded the file in Part 2, use:\n",
    "df = pd.read_csv('bert_classification_dataset.csv')\n",
    "\n",
    "# Or if you saved it to Drive, use:\n",
    "# df = pd.read_csv('/content/drive/MyDrive/legal_bert_classification/bert_classification_dataset.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of unique labels: {df['label'].nunique()}\")\n",
    "\n",
    "# Display label distribution\n",
    "print(\"Label distribution:\")\n",
    "label_counts = df['label'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot label distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "plt.title('Document Type Distribution')\n",
    "plt.xlabel('Document Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Text length analysis\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "print(\"\\nText length statistics:\")\n",
    "print(df['text_length'].describe())\n",
    "\n",
    "# Plot text length distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='text_length', hue='label', bins=50, element='step')\n",
    "plt.title('Text Length Distribution by Document Type')\n",
    "plt.xlabel('Text Length (characters)')\n",
    "plt.xlim(0, df['text_length'].quantile(0.99))  # Limit x-axis to 99th percentile\n",
    "plt.legend(title='Document Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample document display\n",
    "print(\"\\nSample document from each class:\")\n",
    "for label in df['label'].unique():\n",
    "    sample = df[df['label'] == label].iloc[0]\n",
    "    print(f\"\\n--- {label} Example ---\")\n",
    "    print(f\"Text (first 300 chars): {sample['text'][:300]}...\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Legal Document Classification with BERT - Part 2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
